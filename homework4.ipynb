{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание № 4. Языковые модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1 (8 баллов)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В семинаре для генерации мы использовали предположение маркова и считали, что слово зависит только от 1 предыдущего слова. Но ничто нам не мешает попробовать увеличить размер окна и учитывать два или даже три прошлых слова. Для них мы еще сможем собрать достаточно статистик и, логично предположить, что качество сгенерированного текста должно вырасти."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуйте сделать языковую модель, которая будет учитывать два предыдущих слова при генерации текста.\n",
    "Сгенерируйте несколько текстов (3-5) и расчитайте перплексию получившейся модели. \n",
    "Можно использовать данные из семинара или любые другие (сопоставимые или большие по объему). Перплексию рассчитывайте на 10-50 отложенных предложениях (они не должны использоваться при сборе статистик).\n",
    "\n",
    "\n",
    "Подсказки:  \n",
    "    - нужно будет добавить еще один тэг <start>  \n",
    "    - еще одна матрица не нужна, можно по строкам хронить биграмы, а по колонкам униграммы  \n",
    "    - тексты должны быть очень похожи на нормальные (если у вас получается рандомная каша, вы что-то делаете не так). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T19:36:53.699483Z",
     "start_time": "2021-12-15T19:36:48.649388Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from razdel import sentenize\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "from nltk.tokenize import sent_tokenize\n",
    "from scipy.sparse import csr_matrix, csc_matrix, lil_matrix\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T19:37:52.383908Z",
     "start_time": "2021-12-15T19:36:53.705438Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "dvach = open('2ch_corpus.txt', encoding = 'utf-8').read()\n",
    "def normalize(text):\n",
    "    normalized_text = [word.text.strip(punctuation) for word \\\n",
    "                                                            in razdel_tokenize(text)]\n",
    "    normalized_text = [word.lower() for word in normalized_text if word and len(word) < 20 ]\n",
    "    return normalized_text\n",
    "norm_dvach = normalize(dvach)\n",
    "vocab_dvach = Counter(norm_dvach)\n",
    "probas_dvach = Counter({word:c/len(norm_dvach) for word, c in vocab_dvach.items()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T19:39:03.857379Z",
     "start_time": "2021-12-15T19:37:52.394716Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences_dvach = [['<start>']+['<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(dvach)]\n",
    "sentences_dvach_test = sentences_dvach[:50]\n",
    "sentences_dvach = sentences_dvach[50:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T19:39:03.919806Z",
     "start_time": "2021-12-15T19:39:03.863221Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ngrammer(tokens, n=2):\n",
    "    ngrams = []\n",
    "    for i in range(0,len(tokens)-n+1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T19:39:17.640193Z",
     "start_time": "2021-12-15T19:39:03.928084Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigrams_dvach = Counter()\n",
    "bigrams_dvach = Counter()\n",
    "unigrams_dvach = Counter()\n",
    "\n",
    "\n",
    "for sentence in sentences_dvach:\n",
    "    trigrams_dvach.update(ngrammer(sentence,3))\n",
    "    bigrams_dvach.update(ngrammer(sentence,2))\n",
    "    unigrams_dvach.update(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T19:39:52.408342Z",
     "start_time": "2021-12-15T19:39:17.644957Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matrix_dvach = lil_matrix((len(bigrams_dvach), len(unigrams_dvach)))\n",
    "\n",
    "id2word_dvach = list(unigrams_dvach)\n",
    "word2id_dvach = {word:i for i, word in enumerate(id2word_dvach)}\n",
    "\n",
    "id2bigram_dvach = list(bigrams_dvach)\n",
    "bigram2id_dvach = {word:i for i, word in enumerate(id2bigram_dvach)}\n",
    "\n",
    "for ngram in trigrams_dvach:\n",
    "    word1, word2, word3 = ngram.split()\n",
    "    bigram = word1 + ' '+ word2 \n",
    "\n",
    "    matrix_dvach[bigram2id_dvach[bigram], word2id_dvach[word3]] =  (trigrams_dvach[ngram]/bigrams_dvach[bigram])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T19:39:56.284538Z",
     "start_time": "2021-12-15T19:39:52.416290Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matrix_dvach = csr_matrix(matrix_dvach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T19:39:56.346711Z",
     "start_time": "2021-12-15T19:39:56.295900Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate(matrix, id2bigram, id2word, bigram2id, n=100, start='<start> <start>'):\n",
    "    text = []\n",
    "    current_idx = bigram2id[start]\n",
    "    for i in range(n):\n",
    "        p = matrix[current_idx].toarray()[0]\n",
    "        chosen = np.random.choice(list(range(matrix.shape[1])), p = p)\n",
    "        text.append(id2word[chosen])\n",
    "        if id2word[chosen] == '<end>':\n",
    "            current_idx = bigram2id[start]\n",
    "        else:\n",
    "            part = id2bigram[current_idx] + ' ' + id2word[chosen]\n",
    "            part = ' '.join(part.split()[1:])\n",
    "            current_idx = bigram2id[part]\n",
    "    \n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T19:40:02.785244Z",
     "start_time": "2021-12-15T19:39:56.351206Z"
    },
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "я не понимаю \n",
      " 50 \n",
      " как же не сможет \n",
      " похуй на вас и давай быковать помню этот момент в этом треде \n",
      " выйдя из школы фото с военной точки зрения логического аппарата \n",
      " есть что-то кроме пожирания ведер делать \n",
      " к экзаменам тратит время на вглядывание в мелкие детали реалистичное поведение болванчиков и грамотно выстроенный сюжет \n",
      " да не по кайфу тебе это кажется оптимальным размером \n",
      " подобрал беретту с глушителем когда я с радостью сядут \n",
      " ну а никто ничего не могут в окна твоего каркасника заглянет солнце и станет встречаться с качком-альфачом обнимаются-целуются \n",
      " протомолекула противоречит\n"
     ]
    }
   ],
   "source": [
    "print(generate(matrix_dvach, id2bigram_dvach, id2word_dvach, bigram2id_dvach).replace('<end>', '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T19:40:09.380541Z",
     "start_time": "2021-12-15T19:40:02.791201Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "а теперь вуапрос есть ли во мне самом пышно зацвели эти красные дикие цветы и все \n",
      " вы же не уебки какие то жопы да сиськи ищешь \n",
      " если этого значения даже нет я говорю когда вижу в этих фильмах какие-то охуенные концовки \n",
      " иди нахуй мудак ты ничего не потеряло бы \n",
      " дактиль это когда \n",
      " какие есть годные козыри которые сложно повторить но не у меня с кем-то другим потому что вся школа сбежалась до того как они начнут всячески первым срать в унитаз \n",
      " например блядь создатель масс эффекта \n",
      " знаешь ты а как по-другому в современном\n"
     ]
    }
   ],
   "source": [
    "print(generate(matrix_dvach, id2bigram_dvach, id2word_dvach, bigram2id_dvach).replace('<end>', '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T19:40:17.190809Z",
     "start_time": "2021-12-15T19:40:09.389177Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "а что делать \n",
      " ты пришел интересоваться мнениемшто ты максималист дохуя \n",
      " четыре годика \n",
      " что думаете \n",
      " может тут еще бомжи играющие на птр без денег а для вытягиваться корелляций этого высказывания с другими мужиками \n",
      " там объяснили бы но нет той самой прилежной работы без затраченного времени и 20 за равноправие полов \n",
      " основной потребитель батарейки по прежнему котируют \n",
      " и не примелькался его дизайн как у неё небритая пизда бимп 1 \n",
      " я даже не парился да \n",
      " услуги юриста оплатим мы как двач \n",
      " так они пришли за профсоюзными активистами я молчал \n",
      " отличный тред в\n"
     ]
    }
   ],
   "source": [
    "print(generate(matrix_dvach, id2bigram_dvach, id2word_dvach, bigram2id_dvach).replace('<end>', '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T19:40:24.102652Z",
     "start_time": "2021-12-15T19:40:17.196374Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "откуда то чтобы отсосать у него в кармане \n",
      " крутится нужно уметь врать и боюсь \n",
      " после прошлого \n",
      " он капает по галимым 2 в наглую знакомишься потом смело идешь на работу или крутиться вхолостую уже зависит от взгляда на оп-пик не попал в чс сейчас ищу какие-то рукотворные поделия из силиконки но пока он в дурке никто не любит фемифашисток \n",
      " кроме того если бы меня убил так как будто автор сам называет свою игру как будто мне сильно нравится \n",
      " год примерно 5 месяцев из тебя тролль приемлемо \n",
      " да есть она прыгнула но не на открытых участках\n"
     ]
    }
   ],
   "source": [
    "print(generate(matrix_dvach, id2bigram_dvach, id2word_dvach, bigram2id_dvach).replace('<end>', '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T19:40:24.165622Z",
     "start_time": "2021-12-15T19:40:24.116668Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perplexity(probas):\n",
    "    p = np.exp(np.sum(probas))\n",
    "    N = len(probas)\n",
    "    \n",
    "    return p**(-1/N) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T19:40:24.226492Z",
     "start_time": "2021-12-15T19:40:24.173568Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "all_perplexity = []\n",
    "for sent in sentences_dvach_test:\n",
    "    probs = []\n",
    "    for ngram in ngrammer(sent, 3):\n",
    "        word1, word2, word3 = ngram.split()\n",
    "        bigram = word1 + ' '+word2\n",
    "        \n",
    "        if ngram in trigrams_dvach and bigram in bigrams_dvach:\n",
    "            probs.append(np.log(trigrams_dvach[ngram]/bigrams_dvach[bigram]))\n",
    "        else:\n",
    "            probs.append(np.log(0.00001))\n",
    "    if perplexity(probs)!= np.inf: \n",
    "        all_perplexity.append(perplexity(probs))\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T19:40:24.273280Z",
     "start_time": "2021-12-15T19:40:24.232861Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30041.809295252242"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(all_perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание № 2* (2 балла). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прочитайте главу про языковое моделирование в книге Журафски и Мартина - https://web.stanford.edu/~jurafsky/slp3/3.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Развернуто (в пределах 1000 знаков) ответьте на вопросы (по-русски):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Что можно делать с проблемой несловарных слов? В семинаре мы просто использовали какое-то маленькое значение вероятности, а какие есть другие способы?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Для несловарных слов можно добавить псевдослово <UNK> в тестовую выборку.\n",
    "Есть два способа обучить модели с несловарными словами:\n",
    "\n",
    "1. Превратить в задачу с фиксированным словарем, для этого заранее выбирается список слов. В обучающей выборке любое слово, не входящее в словарь конвертируется в токен  <UNK> на этапе нормализации текста. Затем для него оцениваются вероятности, как для любого другого слова в тексте.\n",
    "2. Когда нет фиксированного словаря можно создать его неявно: заменить часть слов на токен <UNK> на основе частотности. Например, заменить все слова, которые встречаются не более чем n раз (n - маленькое число). Или же самостоятельно определить размер словаря, выбрать оттуда какой-то топ (топ-10000) по частоте встречаемости и все остальное заменить на <UNK>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Что такое сглаживание (smoothing)?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Иногда слова уже имеющиеся в словаре могут появиться в тесте в каком-то другом контексте, которого никогда не было в обучающей выборке (например, это слово появилось после того, которое никогда не стояло рядом с ним при обучении). Для того, чтобы избежать нулевой вероятности в таком случае, используют сглаживание, призванное понизить более частотные события в пользу неизвестных до этого событий.\n",
    "Есть несколько типов алгоритмов:\n",
    "Самый простой — сглаживание Лапласа. Оно заключается в том, что к каждому числу нграмм прибавляется единица. \n",
    "Помимо него существуют сглаживание add-k, откат и сглаживание Кнесера-Нея. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
