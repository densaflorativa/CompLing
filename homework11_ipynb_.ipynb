{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1dba7c0d",
      "metadata": {
        "id": "1dba7c0d"
      },
      "source": [
        "# Домашнее задание № 11. Машинный перевод"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Yj7aripVIsbG",
      "metadata": {
        "id": "Yj7aripVIsbG"
      },
      "source": [
        "## Задание 1 (6 баллов + 2 доп балла).\n",
        "Нужно обучить трансформер на этом же или на другом корпусе (можно взять другую языковую пару с того же сайте) и оценивать его на всей тестовой выборке (а не на 10 примерах как сделал я). \n",
        "\n",
        "Чтобы получить 2 доп балла вам нужно будет придумать как оптимизировать функцию translate. Подсказка: модель может предсказывать батчами.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-it/opus.en-it-train.it\n",
        "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-it/opus.en-it-train.en\n",
        "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-it/opus.en-it-test.it\n",
        "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-it/opus.en-it-test.en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1VayheSm4DT",
        "outputId": "26de578f-a7d6-4ce0-c7f5-682eaf6dcf44"
      },
      "id": "o1VayheSm4DT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-22 13:39:13--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-it/opus.en-it-train.it\n",
            "Resolving data.statmt.org (data.statmt.org)... 129.215.197.184\n",
            "Connecting to data.statmt.org (data.statmt.org)|129.215.197.184|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 60742863 (58M)\n",
            "Saving to: ‘opus.en-it-train.it’\n",
            "\n",
            "opus.en-it-train.it 100%[===================>]  57.93M  22.3MB/s    in 2.6s    \n",
            "\n",
            "2022-06-22 13:39:16 (22.3 MB/s) - ‘opus.en-it-train.it’ saved [60742863/60742863]\n",
            "\n",
            "--2022-06-22 13:39:16--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-it/opus.en-it-train.en\n",
            "Resolving data.statmt.org (data.statmt.org)... 129.215.197.184\n",
            "Connecting to data.statmt.org (data.statmt.org)|129.215.197.184|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 56911423 (54M)\n",
            "Saving to: ‘opus.en-it-train.en’\n",
            "\n",
            "opus.en-it-train.en 100%[===================>]  54.27M  21.4MB/s    in 2.5s    \n",
            "\n",
            "2022-06-22 13:39:19 (21.4 MB/s) - ‘opus.en-it-train.en’ saved [56911423/56911423]\n",
            "\n",
            "--2022-06-22 13:39:19--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-it/opus.en-it-test.it\n",
            "Resolving data.statmt.org (data.statmt.org)... 129.215.197.184\n",
            "Connecting to data.statmt.org (data.statmt.org)|129.215.197.184|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 148186 (145K)\n",
            "Saving to: ‘opus.en-it-test.it’\n",
            "\n",
            "opus.en-it-test.it  100%[===================>] 144.71K   485KB/s    in 0.3s    \n",
            "\n",
            "2022-06-22 13:39:20 (485 KB/s) - ‘opus.en-it-test.it’ saved [148186/148186]\n",
            "\n",
            "--2022-06-22 13:39:20--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-it/opus.en-it-test.en\n",
            "Resolving data.statmt.org (data.statmt.org)... 129.215.197.184\n",
            "Connecting to data.statmt.org (data.statmt.org)|129.215.197.184|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 138843 (136K)\n",
            "Saving to: ‘opus.en-it-test.en’\n",
            "\n",
            "opus.en-it-test.en  100%[===================>] 135.59K   452KB/s    in 0.3s    \n",
            "\n",
            "2022-06-22 13:39:20 (452 KB/s) - ‘opus.en-it-test.en’ saved [138843/138843]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers matplotlib sklearn\n"
      ],
      "metadata": {
        "id": "BTlAfRGW5xHK",
        "outputId": "fca86def-5d2d-44e0-ac9b-37630c410ba4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "BTlAfRGW5xHK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tokenizers\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.21.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.4.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.1.0)\n",
            "Installing collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.12.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
        "from string import punctuation\n",
        "from collections import Counter\n",
        "from IPython.display import Image\n",
        "from IPython.core.display import HTML \n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "slys9fOLH3ZB"
      },
      "id": "slys9fOLH3ZB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_sents = open('opus.en-it-train.en').read().splitlines()\n",
        "it_sents = open('opus.en-it-train.it').read().splitlines()"
      ],
      "metadata": {
        "id": "yGSJbQHJI5Py"
      },
      "id": "yGSJbQHJI5Py",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(en_sents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8mXRNa2w9LX",
        "outputId": "105efacf-cb55-43a7-ce4f-6705ec37f6fe"
      },
      "id": "a8mXRNa2w9LX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000000"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(it_sents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKaMYCE0xFye",
        "outputId": "97a9922f-3c17-4386-f29d-d027ff204dd2"
      },
      "id": "DKaMYCE0xFye",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000000"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "en_sents[-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NW86FVH5xSRX",
        "outputId": "c9d878b0-a537-47aa-b49b-18c7ffa25487"
      },
      "id": "NW86FVH5xSRX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Great!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "it_sents[-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qjZ0WD0BxVKd",
        "outputId": "b143c8d9-5238-4b9f-d11d-3b6135d80add"
      },
      "id": "qjZ0WD0BxVKd",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Molto bene.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_en = Tokenizer(BPE())\n",
        "tokenizer_en.pre_tokenizer = Whitespace()\n",
        "trainer_en = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
        "tokenizer_en.train(files=[\"opus.en-it-train.en\"], trainer=trainer_en)\n",
        "\n",
        "tokenizer_it = Tokenizer(BPE())\n",
        "tokenizer_it.pre_tokenizer = Whitespace()\n",
        "trainer_it = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
        "tokenizer_it.train(files=[\"opus.en-it-train.it\"], trainer=trainer_it)"
      ],
      "metadata": {
        "id": "XhN80Hs3YbDI"
      },
      "id": "XhN80Hs3YbDI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_en.save('tokenizer_en')\n",
        "tokenizer_it.save('tokenizer_it')"
      ],
      "metadata": {
        "id": "9jFKG82sbyC0"
      },
      "id": "9jFKG82sbyC0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_en = Tokenizer.from_file(\"tokenizer_en\")\n",
        "tokenizer_it = Tokenizer.from_file(\"tokenizer_it\")"
      ],
      "metadata": {
        "id": "UAjPeDAdbzAN"
      },
      "id": "UAjPeDAdbzAN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(text, tokenizer, max_len):\n",
        "    return [tokenizer.token_to_id('[CLS]')] + tokenizer.encode(text).ids[:max_len] + [tokenizer.token_to_id('[SEP]')]"
      ],
      "metadata": {
        "id": "QInFsmdIb-BZ"
      },
      "id": "QInFsmdIb-BZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PAD_IDX = tokenizer_it.token_to_id('[PAD]')\n",
        "PAD_IDX"
      ],
      "metadata": {
        "id": "HA-nWnmVcgFG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f9594c6-9c52-43bf-a8e1-a82a169d1414"
      },
      "id": "HA-nWnmVcgFG",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len_en, max_len_it = 50, 50"
      ],
      "metadata": {
        "id": "uLx_1enTclo1"
      },
      "id": "uLx_1enTclo1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_en = [encode(t, tokenizer_en, max_len_en) for t in en_sents]\n",
        "X_it = [encode(t, tokenizer_it, max_len_it) for t in it_sents]"
      ],
      "metadata": {
        "id": "c6mptChrcoyS"
      },
      "id": "c6mptChrcoyS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_en)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ao5p7BOU2zzy",
        "outputId": "1e8c4425-61b2-481e-b01e-8b76ac96ea3f"
      },
      "id": "ao5p7BOU2zzy",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000000"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_it)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZQur69s3and",
        "outputId": "6efe28e4-4c5c-44ed-8e75-20c9a52a904c"
      },
      "id": "yZQur69s3and",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000000"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, texts_en, texts_it):\n",
        "        self.texts_en = [torch.LongTensor(sent) for sent in texts_en]\n",
        "        self.texts_en = torch.nn.utils.rnn.pad_sequence(self.texts_en, padding_value=PAD_IDX)\n",
        "        \n",
        "        self.texts_it = [torch.LongTensor(sent) for sent in texts_it]\n",
        "        self.texts_it = torch.nn.utils.rnn.pad_sequence(self.texts_it, padding_value=PAD_IDX)\n",
        "\n",
        "        self.length = len(texts_en)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        ids_en = self.texts_en[:, index]\n",
        "        ids_it = self.texts_it[:, index]\n",
        "\n",
        "        return ids_en, ids_it"
      ],
      "metadata": {
        "id": "_Dqz1XnYc7Hz"
      },
      "id": "_Dqz1XnYc7Hz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_en_train, X_en_valid, X_it_train, X_it_valid = train_test_split(X_en, X_it, test_size=0.05)"
      ],
      "metadata": {
        "id": "A1Jz2z0UdRJp"
      },
      "id": "A1Jz2z0UdRJp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_set = Dataset(X_en_train, X_it_train)\n",
        "training_generator = torch.utils.data.DataLoader(training_set, batch_size=200, shuffle=True, )"
      ],
      "metadata": {
        "id": "Pol-Gw6GdUoJ"
      },
      "id": "Pol-Gw6GdUoJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_set = Dataset(X_en_valid, X_it_valid)\n",
        "valid_generator = torch.utils.data.DataLoader(valid_set, batch_size=200, shuffle=True)"
      ],
      "metadata": {
        "id": "tDbG5jplddFD"
      },
      "id": "tDbG5jplddFD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "e15bQdQIn3H8"
      },
      "id": "e15bQdQIn3H8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer (from [pytorch tutorial](https://pytorch.org/tutorials/beginner/transformer_tutorial.html))"
      ],
      "metadata": {
        "id": "KNMHN3-9ADf-"
      },
      "id": "KNMHN3-9ADf-"
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import Tensor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Transformer\n",
        "import math\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 emb_size: int,\n",
        "                 dropout: float,\n",
        "                 maxlen: int = 150):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: Tensor):\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
        "\n",
        "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "# Seq2Seq Network\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_encoder_layers: int,\n",
        "                 num_decoder_layers: int,\n",
        "                 emb_size: int,\n",
        "                 nhead: int,\n",
        "                 src_vocab_size: int,\n",
        "                 tgt_vocab_size: int,\n",
        "                 dim_feedforward: int = 512,\n",
        "                 dropout: float = 0.1):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "        self.transformer = Transformer(d_model=emb_size, \n",
        "                                       nhead=nhead,\n",
        "                                       num_encoder_layers=num_encoder_layers,\n",
        "                                       num_decoder_layers=num_decoder_layers,\n",
        "                                       dim_feedforward=dim_feedforward,\n",
        "                                       dropout=dropout)\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            emb_size, dropout=dropout)\n",
        "\n",
        "    def forward(self,\n",
        "                src: Tensor,\n",
        "                trg: Tensor,\n",
        "                src_mask: Tensor,\n",
        "                tgt_mask: Tensor,\n",
        "                src_padding_mask: Tensor,\n",
        "                tgt_padding_mask: Tensor,\n",
        "                memory_key_padding_mask: Tensor):\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "#         print('pos inp')\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "#         print('pos dec')\n",
        "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
        "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
        "#         print('pos out')\n",
        "        x = self.generator(outs)\n",
        "#         print('gen')\n",
        "        return x\n",
        "\n",
        "    def encode(self, src: Tensor, src_mask: Tensor):\n",
        "        return self.transformer.encoder(self.positional_encoding(\n",
        "                            self.src_tok_emb(src)), src_mask)\n",
        "\n",
        "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
        "        return self.transformer.decoder(self.positional_encoding(\n",
        "                          self.tgt_tok_emb(tgt)), memory,\n",
        "                          tgt_mask)\n",
        "# During training, we need a subsequent word mask that will prevent model to look into the future words when making predictions. We will also need masks to hide source and target padding tokens. Below, let’s define a function that will take care of both.\n",
        "\n",
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "\n",
        "def create_mask(src, tgt):\n",
        "    src_seq_len = src.shape[0]\n",
        "    tgt_seq_len = tgt.shape[0]\n",
        "\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
        "\n",
        "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
        "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
        "    \n",
        "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
      ],
      "metadata": {
        "id": "Ghf-rAuNAC3m"
      },
      "id": "Ghf-rAuNAC3m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time\n",
        "def train(model, iterator, optimizer, criterion, print_every=500):\n",
        "    \n",
        "    epoch_loss = []\n",
        "    ac = []\n",
        "    \n",
        "    model.train()  \n",
        "\n",
        "    for i, (texts_en, texts_it) in enumerate(iterator):\n",
        "        texts_en = texts_en.T.to(DEVICE) \n",
        "        texts_it = texts_it.T.to(DEVICE) \n",
        "        \n",
        "\n",
        "        texts_it_input = texts_it[:-1, :]\n",
        "        \n",
        "\n",
        "        (texts_en_mask, texts_it_mask, \n",
        "        texts_en_padding_mask, texts_it_padding_mask) = create_mask(texts_en, texts_it_input)\n",
        "        logits = model(texts_en, texts_it_input, texts_en_mask, texts_it_mask,\n",
        "                       texts_en_padding_mask, texts_it_padding_mask, texts_en_padding_mask)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        texts_it_out = texts_it[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), texts_it_out.reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss.append(loss.item())\n",
        "        \n",
        "        if not (i+1) % print_every:\n",
        "            print(f'Loss: {np.mean(epoch_loss)};')\n",
        "        \n",
        "    return np.mean(epoch_loss)\n",
        "\n",
        "\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = []\n",
        "    epoch_f1 = []\n",
        "\n",
        "    model.eval()\n",
        "    for i, (texts_en, texts_it) in enumerate(iterator):\n",
        "        texts_en = texts_en.T.to(DEVICE) \n",
        "        texts_it = texts_it.T.to(DEVICE)\n",
        "        texts_it_input = texts_it[:-1, :]\n",
        "\n",
        "        \n",
        "        (texts_en_mask, texts_it_mask, \n",
        "        texts_en_padding_mask, texts_it_padding_mask) = create_mask(texts_en, texts_it_input)\n",
        "        logits = model(texts_en, texts_it_input, texts_en_mask, texts_it_mask,\n",
        "                       texts_en_padding_mask, texts_it_padding_mask, texts_en_padding_mask)\n",
        "        \n",
        "        texts_it_out = texts_it[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), texts_it_out.reshape(-1))\n",
        "        epoch_loss.append(loss.item())\n",
        "        \n",
        "        \n",
        "    return np.mean(epoch_loss)"
      ],
      "metadata": {
        "id": "zwcWWA9tAcWu"
      },
      "id": "zwcWWA9tAcWu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "EN_VOCAB_SIZE = tokenizer_en.get_vocab_size()\n",
        "IT_VOCAB_SIZE = tokenizer_it.get_vocab_size()\n",
        "\n",
        "EMB_SIZE = 256\n",
        "NHEAD = 8\n",
        "FFN_HID_DIM = 512\n",
        "NUM_ENCODER_LAYERS = 2\n",
        "NUM_DECODER_LAYERS = 2\n",
        "\n",
        "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
        "                                 NHEAD, EN_VOCAB_SIZE, IT_VOCAB_SIZE, FFN_HID_DIM)\n",
        "\n",
        "for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "transformer = transformer.to(DEVICE)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX).to(DEVICE)\n",
        "\n",
        "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
      ],
      "metadata": {
        "id": "agC_xwJWAcTy"
      },
      "id": "agC_xwJWAcTy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.save(transformer, 'model')"
      ],
      "metadata": {
        "id": "VbHmOawgAcP8"
      },
      "id": "VbHmOawgAcP8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#transformer = torch.load('model').to(DEVICE)"
      ],
      "metadata": {
        "id": "wGk3JFKzAcLh"
      },
      "id": "wGk3JFKzAcLh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "JBhn_RLSAcGx"
      },
      "id": "JBhn_RLSAcGx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import default_timer as timer\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "losses = []\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    start_time = timer()\n",
        "    train_loss = train(transformer, training_generator, optimizer, loss_fn)\n",
        "    end_time = timer()\n",
        "    val_loss = evaluate(transformer, valid_generator, loss_fn)\n",
        "    \n",
        "    if not losses:\n",
        "        print(f'First epoch - {val_loss}, saving model..')\n",
        "        torch.save(transformer, 'model')\n",
        "    \n",
        "    elif val_loss < min(losses):\n",
        "        print(f'Improved from {min(losses)} to {val_loss}, saving model..')\n",
        "        torch.save(transformer, 'model')\n",
        "    \n",
        "    losses.append(val_loss)\n",
        "        \n",
        "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \\\n",
        "           \"f\"Epoch time={(end_time-start_time):.3f}s\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyc-IcFfAcDr",
        "outputId": "cdd823ad-4483-4a97-ed8f-3be0f49245dc"
      },
      "id": "iyc-IcFfAcDr",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 7.309024249076844;\n",
            "Loss: 6.658414598941803;\n",
            "Loss: 6.307060996055603;\n",
            "Loss: 6.064766675949096;\n",
            "Loss: 5.880776762390137;\n",
            "Loss: 5.728973263422648;\n",
            "Loss: 5.6017643765040805;\n",
            "Loss: 5.490584200382233;\n",
            "Loss: 5.391606550746494;\n",
            "First epoch - 4.363163595199585, saving model..\n",
            "Epoch: 1, Train loss: 5.345, Val loss: 4.363,            Epoch time=1225.000s\n",
            "Loss: 4.423740203857422;\n",
            "Loss: 4.377890146493912;\n",
            "Loss: 4.3411755757331845;\n",
            "Loss: 4.300256332039833;\n",
            "Loss: 4.265555804729462;\n",
            "Loss: 4.229944823582967;\n",
            "Loss: 4.196107588699886;\n",
            "Loss: 4.1643239600658415;\n",
            "Loss: 4.131228025595347;\n",
            "Improved from 4.363163595199585 to 3.658487998008728, saving model..\n",
            "Epoch: 2, Train loss: 4.116, Val loss: 3.658,            Epoch time=1230.150s\n",
            "Loss: 3.742268993854523;\n",
            "Loss: 3.72279799413681;\n",
            "Loss: 3.702446423371633;\n",
            "Loss: 3.6838565759658812;\n",
            "Loss: 3.668907503890991;\n",
            "Loss: 3.6502868983745573;\n",
            "Loss: 3.6321085269791737;\n",
            "Loss: 3.615466219186783;\n",
            "Loss: 3.597787172847324;\n",
            "Improved from 3.658487998008728 to 3.2695125064849853, saving model..\n",
            "Epoch: 3, Train loss: 3.590, Val loss: 3.270,            Epoch time=1229.720s\n",
            "Loss: 3.345291443824768;\n",
            "Loss: 3.339761175394058;\n",
            "Loss: 3.3309725681940714;\n",
            "Loss: 3.3244650708436967;\n",
            "Loss: 3.314824819278717;\n",
            "Loss: 3.303846442143122;\n",
            "Loss: 3.29431266273771;\n",
            "Loss: 3.283717136323452;\n",
            "Loss: 3.2737471590571934;\n",
            "Improved from 3.2695125064849853 to 3.0416380290985106, saving model..\n",
            "Epoch: 4, Train loss: 3.269, Val loss: 3.042,            Epoch time=1230.012s\n",
            "Loss: 3.094989677906036;\n",
            "Loss: 3.0943590490818025;\n",
            "Loss: 3.0910537643432616;\n",
            "Loss: 3.0849599696397783;\n",
            "Loss: 3.081574195766449;\n",
            "Loss: 3.076922962109248;\n",
            "Loss: 3.0718483648981367;\n",
            "Loss: 3.0663153929710387;\n",
            "Loss: 3.0616218565305076;\n",
            "Improved from 3.0416380290985106 to 2.893811534881592, saving model..\n",
            "Epoch: 5, Train loss: 3.059, Val loss: 2.894,            Epoch time=1230.754s\n",
            "Loss: 2.927161470413208;\n",
            "Loss: 2.930733116149902;\n",
            "Loss: 2.928262832959493;\n",
            "Loss: 2.9262642030715944;\n",
            "Loss: 2.923874115943909;\n",
            "Loss: 2.920631199121475;\n",
            "Loss: 2.9172927477019175;\n",
            "Loss: 2.9150161345005037;\n",
            "Loss: 2.9120381814108955;\n",
            "Improved from 2.893811534881592 to 2.782575897216797, saving model..\n",
            "Epoch: 6, Train loss: 2.910, Val loss: 2.783,            Epoch time=1230.551s\n",
            "Loss: 2.7988999910354613;\n",
            "Loss: 2.7962173645496367;\n",
            "Loss: 2.7989449820518493;\n",
            "Loss: 2.8023726234436035;\n",
            "Loss: 2.8000061104774474;\n",
            "Loss: 2.8004036355813344;\n",
            "Loss: 2.800054552759443;\n",
            "Loss: 2.8002540268301965;\n",
            "Loss: 2.800228143956926;\n",
            "Improved from 2.782575897216797 to 2.712467350959778, saving model..\n",
            "Epoch: 7, Train loss: 2.799, Val loss: 2.712,            Epoch time=1228.666s\n",
            "Loss: 2.701027739048004;\n",
            "Loss: 2.7079813108444215;\n",
            "Loss: 2.709865596135457;\n",
            "Loss: 2.7109942235946654;\n",
            "Loss: 2.7102870279312135;\n",
            "Loss: 2.7117695105075836;\n",
            "Loss: 2.712253982816424;\n",
            "Loss: 2.713139369547367;\n",
            "Loss: 2.71299669429991;\n",
            "Improved from 2.712467350959778 to 2.6521998777389526, saving model..\n",
            "Epoch: 8, Train loss: 2.713, Val loss: 2.652,            Epoch time=1227.445s\n",
            "Loss: 2.6239471364021303;\n",
            "Loss: 2.6360160834789275;\n",
            "Loss: 2.6385834840138753;\n",
            "Loss: 2.6392145035266874;\n",
            "Loss: 2.638804628753662;\n",
            "Loss: 2.640741310755412;\n",
            "Loss: 2.642578641414642;\n",
            "Loss: 2.6448153893351556;\n",
            "Loss: 2.645569965733422;\n",
            "Improved from 2.6521998777389526 to 2.616795882225037, saving model..\n",
            "Epoch: 9, Train loss: 2.646, Val loss: 2.617,            Epoch time=1226.620s\n",
            "Loss: 2.5639090003967286;\n",
            "Loss: 2.5718246059417726;\n",
            "Loss: 2.575980413913727;\n",
            "Loss: 2.57765684068203;\n",
            "Loss: 2.5804195790290834;\n",
            "Loss: 2.5827223489284514;\n",
            "Loss: 2.5864146310942515;\n",
            "Loss: 2.5888792700767516;\n",
            "Loss: 2.58948664188385;\n",
            "Improved from 2.616795882225037 to 2.5793575191497804, saving model..\n",
            "Epoch: 10, Train loss: 2.590, Val loss: 2.579,            Epoch time=1229.523s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(text):\n",
        "\n",
        "\n",
        "    input_ids = [tokenizer_en.token_to_id('[CLS]')] + tokenizer_en.encode(text).ids[:max_len_en] + [tokenizer_en.token_to_id('[SEP]')]\n",
        "    output_ids = [tokenizer_it.token_to_id('[CLS]')]\n",
        "\n",
        "    input_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(input_ids)]).to(DEVICE)\n",
        "    output_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(output_ids)]).to(DEVICE)\n",
        "\n",
        "    (texts_en_mask, texts_it_mask, \n",
        "    texts_en_padding_mask, texts_it_padding_mask) = create_mask(input_ids_pad, output_ids_pad)\n",
        "    logits = transformer(input_ids_pad, output_ids_pad, texts_en_mask, texts_it_mask,\n",
        "                   texts_en_padding_mask, texts_it_padding_mask, texts_en_padding_mask)\n",
        "    pred = logits.argmax(2).item()\n",
        "\n",
        "    while pred not in [tokenizer_it.token_to_id('[SEP]'), tokenizer_it.token_to_id('[PAD]')]:\n",
        "        output_ids.append(pred)\n",
        "        output_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(output_ids)]).to(DEVICE)\n",
        "\n",
        "        (texts_en_mask, texts_it_mask, \n",
        "        texts_en_padding_mask, texts_it_padding_mask) = create_mask(input_ids_pad, output_ids_pad)\n",
        "        logits = transformer(input_ids_pad, output_ids_pad, texts_en_mask, texts_it_mask,\n",
        "                       texts_en_padding_mask, texts_it_padding_mask, texts_en_padding_mask)\n",
        "        pred = logits.argmax(2)[-1].item()\n",
        "\n",
        "    return (' '.join([tokenizer_it.id_to_token(i).replace('##', '') for i in output_ids[1:]]))\n",
        "\n"
      ],
      "metadata": {
        "id": "nCeBvAnNAb_g"
      },
      "id": "nCeBvAnNAb_g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate('Does this function work?')"
      ],
      "metadata": {
        "id": "hyOS9l43FVUo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2d03013b-69c3-442b-8877-3d257f207f3f"
      },
      "id": "hyOS9l43FVUo",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Questa funzione funziona ?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-MbtZmrdAb4C"
      },
      "id": "-MbtZmrdAb4C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BLEU Score"
      ],
      "metadata": {
        "id": "kmJRf8MEF9fQ"
      },
      "id": "kmJRf8MEF9fQ"
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "jXoxLZ3VGA6T"
      },
      "id": "jXoxLZ3VGA6T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_sents_test = open('opus.en-it-test.en').read().lower().splitlines()\n",
        "it_sents_test = open('opus.en-it-test.it').read().lower().splitlines()"
      ],
      "metadata": {
        "id": "nmqJUvgGGA1x"
      },
      "id": "nmqJUvgGGA1x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translations = []\n",
        "\n",
        "for i in range(len(it_sents_test)):\n",
        "  translations.append(translate(it_sents_test[i]))"
      ],
      "metadata": {
        "id": "IyfJJXpWGAsr"
      },
      "id": "IyfJJXpWGAsr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleus = []\n",
        "\n",
        "for i, t in enumerate(translations):\n",
        "  reference = tokenizer_it.encode(t).tokens\n",
        "  hypothesis = tokenizer_it.encode(it_sents_test[i]).tokens\n",
        "\n",
        "bleus.append(nltk.translate.bleu_score.sentence_bleu([reference], hypothesis,  ))"
      ],
      "metadata": {
        "id": "9jJkfvr3GAfv"
      },
      "id": "9jJkfvr3GAfv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleus"
      ],
      "metadata": {
        "id": "tNsInhBZb_iH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4033b752-8619-4cf7-fce8-0b2f9461bbe3"
      },
      "id": "tNsInhBZb_iH",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.056676653300637835]"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xLdRLDW9HZkv"
      },
      "id": "xLdRLDW9HZkv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "e_7yEWNKb_cE"
      },
      "id": "e_7yEWNKb_cE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b5aa93d6",
      "metadata": {
        "id": "b5aa93d6"
      },
      "source": [
        "\n",
        "## Задание 2 (2 балла).\n",
        "Прочитайте главу про машинный перевод у Журафски и Маннига - https://web.stanford.edu/~jurafsky/slp3/10.pdf \n",
        "Ответьте своими словами в чем заключается техника back translation? Для чего она применяется и что позволяет получить? Опишите по шагам как его применить к паре en-ru на данных из семинара. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Back translation -- обычно используется для малорерсурсных языков в случае недостатка данных для составления больших параллельных корпусов, т.е когда мало данных для одного из языков. \n",
        "Модель обучается на существуещих данных, а затем переводится на исходных малоресурсный язык, после чего эти данные добавляются к малоресурсному корпусую\n",
        " \n",
        " Если бы мы взяли мало текстов на английском, но много на русском, мы могли бы использовать этот метод на данных с семинара: обучить трансформер, перевести тестовую выбрку, добавить эти тексты в тренировочную выборку, а затем обучить новый трансформер.\n"
      ],
      "metadata": {
        "id": "4kMSYdrIUSZ8"
      },
      "id": "4kMSYdrIUSZ8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a5bf2f6",
      "metadata": {
        "id": "5a5bf2f6"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "name": "homework11.ipynb\"",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}