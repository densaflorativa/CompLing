{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание № 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T00:44:04.735820Z",
     "start_time": "2022-02-24T00:43:57.542637Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import adagram\n",
    "from lxml import html\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from string import punctuation\n",
    "import json, os, re, sys\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "from matplotlib import pyplot as plt\n",
    "import gensim\n",
    "import pandas as pd\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "morph = MorphAnalyzer()\n",
    "punct = punctuation+'«»—…“”*№–'\n",
    "stops = set(stopwords.words('russian'))\n",
    "\n",
    "def normalize(text):\n",
    "    \n",
    "    words = [token.text.strip(punct) for token in list(razdel_tokenize(text))]\n",
    "    words = [morph.parse(word)[0].normal_form for word in words if word and word not in stops]\n",
    "\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1 Реализовать алгоритм Леска и проверить его на реальном датасете (8 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ворднет можно использовать для дизамбигуации. Самый простой алгоритм дизамбигуации - алгоритм Леска. В нём нужное значение слова находится через пересечение слов контекста, в котором употреблено это слово, с определениями значений слова из ворднета. Значение с максимальным пересечением - нужное."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте его"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T00:44:06.451691Z",
     "start_time": "2022-02-24T00:44:04.738830Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Светлана\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T00:44:06.482691Z",
     "start_time": "2022-02-24T00:44:06.455692Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lesk( word, sentence ):\n",
    "    \"\"\"Ваш код тут\"\"\"\n",
    "    bestsense = 0\n",
    "    maxoverlap = 0\n",
    "    contexts = normalize(sentence)\n",
    "    \n",
    "    synsets = wn.synsets(word)\n",
    "    for i, syns in enumerate(synsets):\n",
    "        definition = syns.definition()\n",
    "        definition = normalize(definition)\n",
    "        overlap = len(list(set(contexts) & set(definition)))\n",
    "        if overlap > maxoverlap:\n",
    "            bestsense = i\n",
    "            maxoverlap = overlap        \n",
    "    return bestsense\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Работать функция должна как-то так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T00:44:10.081365Z",
     "start_time": "2022-02-24T00:44:06.485690Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# на вход подается элемент результата работы уже написанной вами функции get_words_in_context\n",
    "lesk('day', 'some point or period in time') # для примера контекст совпадает с одним из определений\n",
    "# а на выходе индекс подходящего синсета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T00:44:10.097364Z",
     "start_time": "2022-02-24T00:44:10.085368Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'some point or period in time'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# с помощью этого индекса достаем нужный синсет\n",
    "wn.synsets('day')[1].definition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Проверьте насколько хорошо работает такой метод на датасете из семинара**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T00:44:13.851625Z",
     "start_time": "2022-02-24T00:44:10.110362Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_wsd = []\n",
    "corpus = open('corpus_wsd_50k.txt').read().split('\\n\\n')\n",
    "for sent in corpus:\n",
    "    corpus_wsd.append([s.split('\\t') for s in sent.split('\\n')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Корпус состоит из предложений, где у каждого слова три поля - значение, лемма и само слово. Значение пустое, когда слово однозначное, а у многозначных слов стоит тэг вида **'long%3:00:02::'** Это тэг wordnet'ного формата"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T00:45:42.796657Z",
     "start_time": "2022-02-24T00:44:13.858629Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.367619926199262"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true = 0\n",
    "total = 0\n",
    "for sent in corpus_wsd[:1000]:\n",
    "    context = ' '.join([w[-1] for w in sent])\n",
    "    #print(context)\n",
    "    for word in sent:\n",
    "        if word[0] != '':\n",
    "            total+=1\n",
    "            pred = lesk( word[1], context )\n",
    "            if wn.synsets(word[1])[pred] == wn.lemma_from_key(word[0]).synset():\n",
    "                true+=1\n",
    "accuracy = true/total\n",
    "accuracy            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда с помощью алгоритма Леска вы найдете подходящее значение, их можно просто сравнить"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вам нужно для каждого многозначного слова (т.е. у него есть тэг в первом поле) с помощью алгоритма Леска предсказать нужный синсет и сравнить с правильным. Посчитайте процент правильных предсказаний (accuracy).**\n",
    "\n",
    "Если считается слишком долго, возьмите поменьше предложений (например, только тысячу)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2* (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В семинаре для WSI на данных Диалога использовался только Fastext. Попробуйте заменить его на адаграм (обучите свою модель или используйте предобученную out.pkl или https://s3.amazonaws.com/kostia.lopuhin/all.a010.p10.d300.w5.m100.nonorm.slim.joblib), а также поэкспериментируйте с разными алгоритмами кластеризации и их параметрами (можно использовать только те алгоритмы, которые обучаются достаточно быстро)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каждого эксперимента рассчитайте ARI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T00:45:42.940651Z",
     "start_time": "2022-02-24T00:45:42.799663Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import adagram\n",
    "import pandas as pd\n",
    "from sklearn.cluster import *\n",
    "import wget\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T00:45:43.606589Z",
     "start_time": "2022-02-24T00:45:42.943649Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vm = adagram.VectorModel.load(\"out.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T00:45:43.670581Z",
     "start_time": "2022-02-24T00:45:43.612588Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embedding_adagram(text, model, window=5, dim=100):\n",
    "    \n",
    "    \n",
    "    word2context = []\n",
    "    for i in range(len(text)-1):\n",
    "        left = max(0, i-window)\n",
    "        word = text[i]\n",
    "        left_context = text[left:i]\n",
    "        right_context = text[i+1:i+window]\n",
    "        context = left_context + right_context\n",
    "        word2context.append((word, context))\n",
    "    \n",
    "    \n",
    "    \n",
    "    vectors = np.zeros((len(word2context), dim))\n",
    "    \n",
    "    for i,word in enumerate(word2context):\n",
    "        word, context = word\n",
    "        try:\n",
    "            sense = model.disambiguate(word, context).argmax()\n",
    "            v = model.sense_vector(word, sense)\n",
    "            vectors[i] = v \n",
    "\n",
    "        except (KeyError):\n",
    "            continue\n",
    "    \n",
    "    if vectors.any():\n",
    "        vector = np.average(vectors, axis=0)\n",
    "    else:\n",
    "        vector = np.zeros((dim))\n",
    "    \n",
    "    return vector\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T00:45:44.349103Z",
     "start_time": "2022-02-24T00:45:43.676581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0% [                                                                            ]      0 / 365614\r",
      "  2% [.                                                                           ]   8192 / 365614\r",
      "  4% [...                                                                         ]  16384 / 365614\r",
      "  6% [.....                                                                       ]  24576 / 365614\r",
      "  8% [......                                                                      ]  32768 / 365614\r",
      " 11% [........                                                                    ]  40960 / 365614\r",
      " 13% [..........                                                                  ]  49152 / 365614\r",
      " 15% [...........                                                                 ]  57344 / 365614\r",
      " 17% [.............                                                               ]  65536 / 365614\r",
      " 20% [...............                                                             ]  73728 / 365614\r",
      " 22% [.................                                                           ]  81920 / 365614\r",
      " 24% [..................                                                          ]  90112 / 365614\r",
      " 26% [....................                                                        ]  98304 / 365614\r",
      " 29% [......................                                                      ] 106496 / 365614\r",
      " 31% [.......................                                                     ] 114688 / 365614\r",
      " 33% [.........................                                                   ] 122880 / 365614\r",
      " 35% [...........................                                                 ] 131072 / 365614\r",
      " 38% [............................                                                ] 139264 / 365614\r",
      " 40% [..............................                                              ] 147456 / 365614\r",
      " 42% [................................                                            ] 155648 / 365614\r",
      " 44% [..................................                                          ] 163840 / 365614\r",
      " 47% [...................................                                         ] 172032 / 365614\r",
      " 49% [.....................................                                       ] 180224 / 365614\r",
      " 51% [.......................................                                     ] 188416 / 365614\r",
      " 53% [........................................                                    ] 196608 / 365614\r",
      " 56% [..........................................                                  ] 204800 / 365614\r",
      " 58% [............................................                                ] 212992 / 365614\r",
      " 60% [.............................................                               ] 221184 / 365614\r",
      " 62% [...............................................                             ] 229376 / 365614\r",
      " 64% [.................................................                           ] 237568 / 365614\r",
      " 67% [...................................................                         ] 245760 / 365614\r",
      " 69% [....................................................                        ] 253952 / 365614\r",
      " 71% [......................................................                      ] 262144 / 365614\r",
      " 73% [........................................................                    ] 270336 / 365614\r",
      " 76% [.........................................................                   ] 278528 / 365614\r",
      " 78% [...........................................................                 ] 286720 / 365614\r",
      " 80% [.............................................................               ] 294912 / 365614\r",
      " 82% [...............................................................             ] 303104 / 365614\r",
      " 85% [................................................................            ] 311296 / 365614\r",
      " 87% [..................................................................          ] 319488 / 365614\r",
      " 89% [....................................................................        ] 327680 / 365614\r",
      " 91% [.....................................................................       ] 335872 / 365614\r",
      " 94% [.......................................................................     ] 344064 / 365614\r",
      " 96% [.........................................................................   ] 352256 / 365614\r",
      " 98% [..........................................................................  ] 360448 / 365614\r",
      "100% [............................................................................] 365614 / 365614"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'train.csv'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://raw.githubusercontent.com/nlpub/russe-wsi-kit/master/data/main/wiki-wiki/train.csv'\n",
    "wget.download(url, 'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T00:45:44.587078Z",
     "start_time": "2022-02-24T00:45:44.353101Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv', sep='\\t', error_bad_lines=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T00:45:44.619084Z",
     "start_time": "2022-02-24T00:45:44.592082Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grouped_df = df.groupby('word')[['word', 'context', 'gold_sense_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T00:45:44.667076Z",
     "start_time": "2022-02-24T00:45:44.625078Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_ari(cluster):\n",
    "    ARI = []\n",
    "\n",
    "    for key, _ in grouped_df:\n",
    "        texts = grouped_df.get_group(key)['context'].apply(normalize)\n",
    "        X = np.zeros((len(texts), 100))\n",
    "\n",
    "        for i, text in enumerate(texts):\n",
    "            text = [word for word in text if word != key]\n",
    "            X[i] = get_embedding_adagram(text, vm)\n",
    "\n",
    "        \n",
    "        cluster.fit(X)\n",
    "        labels = np.array(cluster.labels_)+1\n",
    "\n",
    "        ARI.append(adjusted_rand_score(grouped_df.get_group(key)['gold_sense_id'], labels))\n",
    "\n",
    "    return(np.mean(ARI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T00:45:51.240088Z",
     "start_time": "2022-02-24T00:45:44.675078Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04266132797411952"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_ari(AffinityPropagation(damping=0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T00:45:56.680774Z",
     "start_time": "2022-02-24T00:45:51.244095Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21416506362837714"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_ari(KMeans(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T00:46:01.549947Z",
     "start_time": "2022-02-24T00:45:56.685777Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0011175070915487569"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_ari(DBSCAN(min_samples=1, eps=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "лучший результат у Kmeans"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
